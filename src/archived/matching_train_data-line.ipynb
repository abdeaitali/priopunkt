{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching trains with traffic lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal here is to develop a function that makes it possible to identify the line of a specific delayed train. The reason why we need such a function is because the passenger ridership estimation is given per line where the delay data is per specific train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us import the dataset for all the traffic lines (used in the ridership estimation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import excel file static_pass_all_2024.xlsx\n",
    "import pandas as pd\n",
    "\n",
    "# read by default 1st sheet of an excel file\n",
    "df_line = pd.read_excel('../../data/output_data/static_pass_all_2024.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all the columns except the first 3 (no need for ridership data, only the line number, name and stopping patterns are of interest)\n",
    "df_line = df_line.iloc[:, :9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now import the train data, more specifically the trains that are affected by delays. Of interest here are particularly Tågnr\tand Tåguppdrag.\n",
    "The goal is to match all of them to a specific line number in df_line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ny datafil med alla RST tåg som har fått registrering av Infrastruktur händelser på södrastambanan\n",
    "df_train = pd.read_csv('../../data/train_data_2023/traindata_2023_passenger_SSBevents_012025.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to clean up (make this df a bit smaller), e.g., by removing unnecessary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_rst = df_train[df_train['Tågslag'] == 'RST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the following columns\n",
    "# 'resa', 'Tågnr', 'Tåguppdrag', 'Plats', 'Tågslag', 'Tågsort', 'Aktivitetskod', 'Aktivitetskodbeskrivning'\n",
    "# 'PlanDatum', 'PlanTidpunkt', 'Datum',  'Tågläge', 'StartStation_resa', 'SlutStation_resa','StartStation_uppdrag', 'SlutStation_uppdrag\n",
    "df_train_rst_clean = df_train_rst[['resa', 'Tågnr', 'Tåguppdrag', 'Plats', 'Tågslag', 'Tågsort', 'Aktivitetskod', 'Aktivitetskodbeskrivning', 'PlanDatum', 'PlanTidpunkt', 'Datum',  'Tågläge', 'StartStation_resa', 'SlutStation_resa','StartStation_uppdrag', 'SlutStation_uppdrag']]\n",
    "df_train_rst_clean = df_train_rst_clean.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all rows where Aktivitetskodbeskrivning is not 'Påstigande av resande', or 'Av- och påstigande av resande' or 'Avstigande av resande'\n",
    "df_train_rst_clean = df_train_rst_clean[df_train_rst_clean['Aktivitetskodbeskrivning'].isin(['Påstigande av resande', 'Av- och påstigande av resande', 'Avstigande av resande'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, clean up df_plats_sign\n",
    "df_plats_sign = pd.read_excel('../../data/useful_data/Förbindelselinje_2023_alla.xlsx')\n",
    "df_plats_sign = df_plats_sign.dropna(subset=['Plats'])\n",
    "df_plats_sign = df_plats_sign[['Plats', 'Plats_sign']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save df_plats_sign to a csv file, keep columns 'Plats' and 'Plats_sign'\n",
    "# df_plats_sign.to_csv('plats_sign_2023_alla.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "# Define the regex matching function\n",
    "def match_station(station, plats_df):\n",
    "    station_variants = [\n",
    "        station,\n",
    "        re.sub(r'central', 'c', station, flags=re.IGNORECASE),\n",
    "        re.sub(r'(\\w+)( central)', r'\\1s central', station, flags=re.IGNORECASE),\n",
    "        re.sub(r'(\\w+)( c)', r'\\1s c', station, flags=re.IGNORECASE),\n",
    "        re.sub(r'(\\w+)s central', r'\\1 central', station, flags=re.IGNORECASE),\n",
    "        re.sub(r'(\\w+)s central', r'\\1 c', station, flags=re.IGNORECASE),\n",
    "    ]\n",
    "    \n",
    "    # if station is Marieholm, try matching Göteborg Marieholm\n",
    "    if station.lower() == 'marieholm':\n",
    "        station_variants.append('Göteborg Marieholm')\n",
    "    # if station is Helsingborg godsbangård, try matching Helsingborgs godsbangård\n",
    "    if station.lower() == 'helsingborg godsbangård':\n",
    "        station_variants.append('Helsingborgs godsbangård')\n",
    "    # if station is Hallsbergs pbg, try matching Hallsbergs personbangård\n",
    "    if station.lower() == 'hallsbergs pbg':\n",
    "        station_variants.append('Hallsbergs personbangård')\n",
    "\n",
    "    # if station is Stockholm Södra, try Stockholms Södra\n",
    "    if station.lower() == 'stockholm södra':\n",
    "        station_variants.append('Stockholms Södra')\n",
    "\n",
    "    # if station is Falkenbergs personstation, try matching Falkenberg personstation\n",
    "    if station.lower() == 'falkenbergs personstation':\n",
    "        station_variants.append('Falkenberg personstation')\n",
    "\n",
    "    # if station is Köpingebro , try matching f.d. Köpingebro\n",
    "    if station.lower() == 'köpingebro':\n",
    "        station_variants.append('f.d. Köpingebro')\n",
    "\n",
    "    for variant in station_variants:\n",
    "        matches = plats_df[plats_df['Plats'].str.match(re.escape(variant), case=False, na=False)]\n",
    "        if not matches.empty:\n",
    "            return matches['Plats_sign'].iloc[0]\n",
    "    return None\n",
    "\n",
    "# Filter activities first\n",
    "df_train_rst_clean = df_train_rst_clean[df_train_rst_clean['Aktivitetskodbeskrivning'].isin([\n",
    "    'Påstigande av resande', \n",
    "    'Av- och påstigande av resande', \n",
    "    'Avstigande av resande'\n",
    "])]\n",
    "\n",
    "# Create initial mapping for station signs\n",
    "df_train_rst_clean = df_train_rst_clean.merge(\n",
    "    df_plats_sign[['Plats', 'Plats_sign']], \n",
    "    left_on='Plats', \n",
    "    right_on='Plats', \n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle unmatched stations\n",
    "unmatched_stations = df_train_rst_clean[df_train_rst_clean['Plats_sign'].isna()]['Plats'].unique()\n",
    "\n",
    "# Apply regex matching for unmatched stations\n",
    "for station in unmatched_stations:\n",
    "    match = match_station(station, df_plats_sign)\n",
    "    if match:\n",
    "        df_train_rst_clean.loc[df_train_rst_clean['Plats'] == station, 'Plats_sign'] = match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the additional station mapping file\n",
    "df_plats_sign_pos = pd.read_excel('../../data/useful_data/Plats_sign_pos.xlsx')\n",
    "\n",
    "# rename column Signatur to Plats_sign\n",
    "df_plats_sign_pos.rename(columns={'Signatur': 'Plats_sign'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third attempt: Regex matching using additional file for remaining unmatched stations\n",
    "unmatched_stations = df_train_rst_clean[df_train_rst_clean['Plats_sign'].isna()]['Plats'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station in unmatched_stations:\n",
    "    match = match_station(station, df_plats_sign_pos)\n",
    "    if match:\n",
    "        df_train_rst_clean.loc[df_train_rst_clean['Plats'] == station, 'Plats_sign'] = match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all station signs to uppercase\n",
    "df_train_rst_clean['Plats_sign'] = df_train_rst_clean['Plats_sign'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the full name for unmatched stations\n",
    "df_train_rst_clean.loc[df_train_rst_clean['Plats_sign'].isna(), 'Plats_sign'] = df_train_rst_clean.loc[df_train_rst_clean['Plats_sign'].isna(), 'Plats']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting stops from train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before trying to find the closest line (line number/name) to a certain train (resa, i.e, tågnr-uppdrag-datum). Let us extract the stops.\n",
    "First, we append the stopping pattern information to our delayed trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with stops from 'Plats' column\n",
    "train_resa_stops = df_train_rst_clean.groupby(['resa', 'Tåguppdrag'])['Plats_sign'].agg(list).reset_index()\n",
    "\n",
    "# If there are duplicates in the 'Plats' lists, we can remove them while preserving order\n",
    "train_resa_stops['Plats_sign'] = train_resa_stops['Plats_sign'].apply(lambda x: list(dict.fromkeys(x)))\n",
    "train_resa_stops['Plats_len'] = train_resa_stops['Plats_sign'].apply(lambda x: len(x))\n",
    "\n",
    "# rename column Plats_sign to Stopps\n",
    "train_resa_stops.rename(columns={'Plats_sign': 'Stopps'}, inplace=True)\n",
    "\n",
    "# Convert lists to tuples to make them hashable\n",
    "train_resa_stops['Stopps'] = train_resa_stops['Stopps'].apply(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a reduced version of train_resa_stops with only Tåguppdrag and Plats while keeping the row with the longest list of stops, call it train_resa_stops_taguppdrag\n",
    "train_resa_stops = train_resa_stops.sort_values(by='Plats_len', ascending=False)\n",
    "\n",
    "# when there are duplicates in 'Tåguppdrag', keep the row with the highest 'Plats_len'\n",
    "train_stops_no_duplicates = train_resa_stops.drop_duplicates(subset=['Tåguppdrag'], keep='first')\n",
    "\n",
    "train_stops_no_duplicates = train_stops_no_duplicates[['resa','Tåguppdrag', 'Stopps']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AbdouAA\\AppData\\Local\\Temp\\ipykernel_17964\\3534091136.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  line_stops = df_line.groupby('Linje').apply(\n"
     ]
    }
   ],
   "source": [
    "# Group by 'Linje' and combine the 'från_sign' and 'till_sign' for each line\n",
    "line_stops = df_line.groupby('Linje').apply(\n",
    "    lambda x: list(x['från_sign']) + [x['till_sign'].iloc[-1]]\n",
    ").reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "line_stops.columns = ['Linje', 'Stopps']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching delayed trains to traffic lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now match delayed trains (subset with unique stop patterns) to the most likely traffic line. The most likely line is chosen as the one with the highest similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def calculate_score(train_stops, line_stops):\n",
    "    \"\"\"\n",
    "    Calculate a similarity score between train stops and line stops.\n",
    "    \"\"\"\n",
    "    # Match first and last stop\n",
    "    score = 0\n",
    "\n",
    "    if train_stops[0] == line_stops[0]:\n",
    "        score += 2  # Higher weight for matching first stop\n",
    "    if train_stops[-1] == line_stops[-1]:\n",
    "        score += 2  # Higher weight for matching last stop\n",
    "    \n",
    "    # Calculate sequence similarity for intermediate stops\n",
    "    sequence_similarity = SequenceMatcher(None, train_stops, line_stops).ratio()\n",
    "    score += sequence_similarity * 10  # Adjust weight for sequence similarity\n",
    "    \n",
    "    return score\n",
    "\n",
    "def get_inverted_line(line_id):\n",
    "    \"\"\"\n",
    "    Get the inverted line ID.\n",
    "    \"\"\"\n",
    "    return line_id[:-1] if line_id.endswith('R') else f\"{line_id}R\"\n",
    "\n",
    "def match_trains_to_lines(train_stops_df, line_stops_df):\n",
    "    \"\"\"\n",
    "    Match trains to lines based on similarity scores, including inverted stops.\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    for _, train_row in train_stops_df.iterrows():\n",
    "        best_score = -1\n",
    "        best_match = None\n",
    "        best_direction = 'Normal'\n",
    "\n",
    "        # if train_row has no stops, set best score to -1 and continue\n",
    "        if len(train_row['Stopps']) > 0:\n",
    "            for _, line_row in line_stops_df.iterrows():\n",
    "\n",
    "                # Calculate score for normal stops\n",
    "                normal_score = calculate_score(train_row['Stopps'], line_row['Stopps'])\n",
    "                \n",
    "                # Calculate score for inverted stops\n",
    "                inverted_stops = line_row['Stopps'][::-1]\n",
    "                inverted_score = calculate_score(train_row['Stopps'], inverted_stops)\n",
    "                \n",
    "                # Determine better match (normal or inverted)\n",
    "                if inverted_score > normal_score:\n",
    "                    current_score = inverted_score\n",
    "                    current_match = get_inverted_line(line_row['Linje'])\n",
    "                    current_direction = 'Inverted'\n",
    "                else:\n",
    "                    current_score = normal_score\n",
    "                    current_match = line_row['Linje']\n",
    "                    current_direction = 'Normal'\n",
    "                \n",
    "                # Update best match\n",
    "                if current_score > best_score:\n",
    "                    best_score = current_score\n",
    "                    best_match = current_match\n",
    "                    best_direction = current_direction\n",
    "        \n",
    "        matches.append({\n",
    "            'resa': train_row['resa'],\n",
    "            'Tåguppdrag': train_row['Tåguppdrag'],\n",
    "            'Predicted_Line': best_match,\n",
    "            'Score': best_score,\n",
    "            'Direction': best_direction\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(matches)\n",
    "\n",
    "\n",
    "matching_result = match_trains_to_lines(train_stops_no_duplicates, line_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to results corresponding stops of the predicted line\n",
    "matching_result_stops = pd.merge(matching_result, line_stops, left_on='Predicted_Line', right_on='Linje', how='left').rename(columns={'Stopps': 'Stopps_line'})\n",
    "matching_result_stops = pd.merge(matching_result_stops, train_stops_no_duplicates, left_on='resa', right_on='resa', how='left').rename(columns={'Stopps': 'Stopps_train'})\n",
    "matching_result_stops.drop(columns=['Linje', 'Tåguppdrag_y'], inplace=True)\n",
    "# rename column Tåguppdrag_x to Tåguppdrag\n",
    "matching_result_stops.rename(columns={'Tåguppdrag_x': 'Tåguppdrag'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can construct the final table where all the trains are identified with a specific traffic line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export matching_result_stops to Excel, keep only the columns 'Tåguppdrag', 'Predicted_Line', 'Score', 'Stopps_line', 'Stopps_train'\n",
    "columns_to_keep = ['Tåguppdrag', 'Predicted_Line', 'Score', 'Stopps_line', 'Stopps_train']\n",
    "matching_result_stops = matching_result_stops[columns_to_keep]\n",
    "matching_result_stops.to_excel('../../data/output_data/matching_result_stops.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
