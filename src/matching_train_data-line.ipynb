{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching trains with traffic lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal here is to develop a function that makes it possible to identify the line of a specific delayed train. The reason why we need such a function is because the passenger ridership estimation is given per line where the delay data is per specific train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us import the dataset for all the traffic lines (used in the ridership estimation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import excel file static_pass_all_2024.xlsx\n",
    "import pandas as pd\n",
    "\n",
    "# read by default 1st sheet of an excel file\n",
    "df_line = pd.read_excel('../data/output_data/static_pass_all_2024.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all the columns except the first 3 (no need for ridership data, only the line number, name and stopping patterns are of interest)\n",
    "df_line = df_line.iloc[:, :9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now import the train data, more specifically the trains that are affected by delays. Of interest here are particularly Tågnr\tand Tåguppdrag.\n",
    "The goal is to match all of them to a specific line number in df_line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ny datafil med alla RST tåg som har fått registrering av Infrastruktur händelser på södrastambanan\n",
    "df_train = pd.read_csv('../data/train_data_2023/traindata_2023_passenger_SSBevents_012025.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to clean up (make this df a bit smaller), e.g., by removing unnecessary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_rst = df_train[df_train['Tågslag'] == 'RST']\n",
    "# upper case for columns 'Plats' 'StartStation_resa', 'SlutStation_resa','StartStation_uppdrag', 'SlutStation_uppdrag\n",
    "df_train_rst.loc[:, 'Plats'] = df_train_rst['Plats'].str.upper()\n",
    "df_train_rst.loc[:,'StartStation_resa'] = df_train_rst['StartStation_resa'].str.upper()\n",
    "df_train_rst.loc[:,'SlutStation_resa'] = df_train_rst['SlutStation_resa'].str.upper()\n",
    "df_train_rst.loc[:,'StartStation_uppdrag'] = df_train_rst['StartStation_uppdrag'].str.upper()\n",
    "df_train_rst.loc[:,'SlutStation_uppdrag'] = df_train_rst['SlutStation_uppdrag'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the following columns\n",
    "# 'resa', 'Tågnr', 'Tåguppdrag', 'Plats', 'Tågslag', 'Tågsort', 'Aktivitetskod', 'Aktivitetskodbeskrivning'\n",
    "# 'PlanDatum', 'PlanTidpunkt', 'Datum',  'Tågläge', 'StartStation_resa', 'SlutStation_resa','StartStation_uppdrag', 'SlutStation_uppdrag\n",
    "df_train_rst_clean = df_train_rst[['resa', 'Tågnr', 'Tåguppdrag', 'Plats', 'Tågslag', 'Tågsort', 'Aktivitetskod', 'Aktivitetskodbeskrivning', 'PlanDatum', 'PlanTidpunkt', 'Datum',  'Tågläge', 'StartStation_resa', 'SlutStation_resa','StartStation_uppdrag', 'SlutStation_uppdrag']]\n",
    "df_train_rst_clean = df_train_rst_clean.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all rows where Aktivitetskodbeskrivning is not 'Påstigande av resande', or 'Av- och påstigande av resande' or 'Avstigande av resande'\n",
    "df_train_rst_clean = df_train_rst_clean[df_train_rst_clean['Aktivitetskodbeskrivning'].isin([\n",
    "    'Påstigande av resande', \n",
    "    'Av- och påstigande av resande', \n",
    "    'Avstigande av resande'\n",
    "])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read ../data/useful_data/Plats_sign.csv\n",
    "df_plats_sign = pd.read_csv('../data/useful_data/Plats_sign.csv')\n",
    "\n",
    "# # First, clean up df_plats_sign\n",
    "# df_plats_sign = pd.read_excel('../data/useful_data/Förbindelselinje_2023_alla.xlsx')\n",
    "# df_plats_sign = df_plats_sign.dropna(subset=['Plats'])\n",
    "# df_plats_sign = df_plats_sign[['Plats', 'Plats_sign']].drop_duplicates()\n",
    "\n",
    "# # Read the additional station mapping file\n",
    "# df_plats_sign_pos = pd.read_excel('../data/useful_data/Plats_sign_pos.xlsx')\n",
    "# # keep only columns Signatur and Plats_sign\n",
    "# df_plats_sign_pos = df_plats_sign_pos[['Plats', 'Signatur']]\n",
    "# # rename column Signatur to Plats_sign\n",
    "# df_plats_sign_pos.rename(columns={'Signatur': 'Plats_sign'}, inplace=True)\n",
    "\n",
    "# # augment df_plats_sign with df_plats_sign_pos\n",
    "# df_plats_sign = pd.concat([df_plats_sign, df_plats_sign_pos], ignore_index=True)\n",
    "\n",
    "# # Read the additional station mapping file\n",
    "# df_trafikplats = pd.read_csv('../data/raw_data/Trafikplats_jvg_förenklad.csv')\n",
    "# # keep only columns trafikplatsnamn and signatur\n",
    "# df_trafikplats = df_trafikplats[['trafikplatsnamn', 'signatur']]\n",
    "# # rename columns\n",
    "# df_trafikplats.rename(columns={'trafikplatsnamn': 'Plats', 'signatur': 'Plats_sign'}, inplace=True)\n",
    "\n",
    "# # augment df_plats_sign with df_trafikplats\n",
    "# df_plats_sign = pd.concat([df_plats_sign, df_trafikplats], ignore_index=True)\n",
    "\n",
    "# # Find the Plats_sign for 'Morastrand'\n",
    "# mora_strand_sign = df_plats_sign[df_plats_sign['Plats'] == 'Morastrand']['Plats_sign'].values[0]\n",
    "\n",
    "# # Create a new row for 'Mora Strand' mapping\n",
    "# new_row = pd.DataFrame({'Plats': ['Mora Strand'], 'Plats_sign': [mora_strand_sign]})\n",
    "\n",
    "# # Concatenate the new row with df_plats_sign\n",
    "# df_plats_sign = pd.concat([df_plats_sign, new_row], ignore_index=True)\n",
    "\n",
    "# df_plats_sign['Plats_sign'] = df_plats_sign['Plats_sign'].str.upper()\n",
    "\n",
    "# # remove duplicates\n",
    "# df_plats_sign = df_plats_sign.drop_duplicates(subset=['Plats_sign', 'Plats'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create initial mapping for station signs\n",
    "df_train_rst_clean = df_train_rst_clean.merge(\n",
    "    df_plats_sign[['Plats', 'Plats_sign']], \n",
    "    left_on='Plats', \n",
    "    right_on='Plats', \n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "# Handle unmatched stations\n",
    "unmatched_stations = df_train_rst_clean[df_train_rst_clean['Plats_sign'].isna()]['Plats'].unique()\n",
    "\n",
    "# Define the regex matching function with df_plats_sign as parameter\n",
    "def match_station(station, df_plats_sign):\n",
    "    \"\"\"\n",
    "    Match station names using regex and handle special cases.\n",
    "    Args:\n",
    "        station: station name to match\n",
    "        df_plats_sign: dataframe containing station mappings\n",
    "    Returns:\n",
    "        matched Plats_sign or None if no match found\n",
    "    \"\"\"\n",
    "    station_variants = [\n",
    "        station,\n",
    "        re.sub(r'central', 'c', station, flags=re.IGNORECASE),\n",
    "        re.sub(r'(\\w+)( central)', r'\\1s central', station, flags=re.IGNORECASE),\n",
    "        re.sub(r'(\\w+)( c)', r'\\1s c', station, flags=re.IGNORECASE),\n",
    "        re.sub(r'(\\w+)s central', r'\\1 central', station, flags=re.IGNORECASE),\n",
    "        re.sub(r'(\\w+)s central', r'\\1 c', station, flags=re.IGNORECASE),\n",
    "    ]\n",
    "        # upper case for all elements in station_variants\n",
    "    station_variants = [x.upper() for x in station_variants]\n",
    "\n",
    "    # Special cases\n",
    "    special_cases = {\n",
    "        'marieholm': 'Göteborg Marieholm',\n",
    "        'helsingborg godsbangård': 'Helsingborgs godsbangård',\n",
    "        'hallsbergs pbg': 'Hallsbergs personbangård',\n",
    "        'stockholm södra': 'Stockholms Södra',\n",
    "        'falkenbergs personstation': 'Falkenberg personstation',\n",
    "        'köpingebro': 'f.d. Köpingebro',\n",
    "        'Mora strand': 'Morastrand'\n",
    "    }\n",
    "\n",
    "    # upper case for all keys in special_cases\n",
    "    special_cases = {k.upper(): v for k, v in special_cases.items()}\n",
    "\n",
    "    \n",
    "    if station.upper() in special_cases:\n",
    "        station_variants.append(special_cases[station.upper()])\n",
    "\n",
    "    # keep only unique elements in station_variants\n",
    "    station_variants = pd.Series(station_variants)\n",
    "\n",
    "    for variant in station_variants:\n",
    "        matches = df_plats_sign[df_plats_sign['Plats'].str.match(re.escape(variant), case=False, na=False)]\n",
    "        if not matches.empty:\n",
    "            # Add the original station name to df_plats_sign if it's not already there\n",
    "            if not df_plats_sign[df_plats_sign['Plats'] == station].shape[0]:\n",
    "                plats_sign = matches['Plats_sign'].iloc[0]\n",
    "                new_row = pd.DataFrame({'Plats': [station], 'Plats_sign': [plats_sign]})\n",
    "                df_plats_sign = pd.concat([df_plats_sign, new_row], ignore_index=True)\n",
    "            return matches['Plats_sign'].iloc[0], df_plats_sign\n",
    "    return None, df_plats_sign\n",
    "\n",
    "# Apply regex matching for unmatched stations\n",
    "for station in unmatched_stations:\n",
    "    match, df_plats_sign = match_station(station, df_plats_sign)\n",
    "    if match:\n",
    "        df_train_rst_clean.loc[df_train_rst_clean['Plats'] == station, 'Plats_sign'] = match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plats_sign = df_plats_sign[['Plats', 'Plats_sign']].drop_duplicates()\n",
    "df_plats_sign.to_csv('../data/useful_data/Plats_sign_augmented_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third attempt: Regex matching using additional file for remaining unmatched stations\n",
    "#unmatched_stations = df_train_rst_clean[df_train_rst_clean['Plats_sign'].isna()]['Plats'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for station in unmatched_stations:\n",
    "#     match = match_station(station, df_plats_sign_pos)\n",
    "#     if match:\n",
    "#         df_train_rst_clean.loc[df_train_rst_clean['Plats'] == station, 'Plats_sign'] = match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the full name for unmatched stations\n",
    "df_train_rst_clean.loc[df_train_rst_clean['Plats_sign'].isna(), 'Plats_sign'] = df_train_rst_clean.loc[df_train_rst_clean['Plats_sign'].isna(), 'Plats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_rst_clean = df_train_rst_clean.merge(\n",
    "    df_plats_sign[['Plats', 'Plats_sign']].rename(columns={'Plats': 'StartStation_uppdrag', 'Plats_sign': 'Start_uppdrag_sign'}),\n",
    "    on='StartStation_uppdrag',\n",
    "    how='left'\n",
    ")\n",
    "df_train_rst_clean.loc[df_train_rst_clean['Plats_sign'].isna(), 'Plats_sign'] = df_train_rst_clean.loc[df_train_rst_clean['Plats_sign'].isna(), 'Plats']\n",
    "\n",
    "# Merge to get Slut_uppdrag_sign\n",
    "df_train_rst_clean = df_train_rst_clean.merge(\n",
    "    df_plats_sign[['Plats', 'Plats_sign']].rename(columns={'Plats': 'SlutStation_uppdrag', 'Plats_sign': 'Slut_uppdrag_sign'}),\n",
    "    on='SlutStation_uppdrag',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge to get Start_resa_sign\n",
    "df_train_rst_clean = df_train_rst_clean.merge(\n",
    "    df_plats_sign[['Plats', 'Plats_sign']].rename(columns={'Plats': 'StartStation_resa', 'Plats_sign': 'Start_resa_sign'}),\n",
    "    on='StartStation_resa',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge to get Slut_resa_sign\n",
    "df_train_rst_clean = df_train_rst_clean.merge(\n",
    "    df_plats_sign[['Plats', 'Plats_sign']].rename(columns={'Plats': 'SlutStation_resa', 'Plats_sign': 'Slut_resa_sign'}),\n",
    "    on='SlutStation_resa',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tåguppdrag per Tågnr:\n",
      "df_train_rst_clean - max: 2, min: 1\n",
      "\n",
      "Number of Tågnr per Tåguppdrag:\n",
      "df_train_rst_clean - max: 19, min: 1\n",
      "\n",
      "Number of Tågnr per Tåguppdrag:\n",
      "df_train_rst_clean - max: 1, min: 1\n",
      "\n",
      "Number of Tågnr per Tåguppdrag:\n",
      "df_train_rst_clean - max: 5, min: 1\n"
     ]
    }
   ],
   "source": [
    "# First part is same as your code\n",
    "x_tgnr = df_train_rst_clean.groupby('Tågnr')['Tåguppdrag'].nunique()\n",
    "print(\"Number of Tåguppdrag per Tågnr:\")\n",
    "print(f\"df_train_rst_clean - max: {x_tgnr.max()}, min: {x_tgnr.min()}\")\n",
    "\n",
    "# Now checking how many Tågnr per Tåguppdrag\n",
    "x_tgupp = df_train_rst_clean.groupby('Tåguppdrag')['Tågnr'].nunique()\n",
    "print(\"\\nNumber of Tågnr per Tåguppdrag:\")\n",
    "print(f\"df_train_rst_clean - max: {x_tgupp.max()}, min: {x_tgupp.min()}\")\n",
    "\n",
    "# Now checking how many Tågnr per Tåguppdrag\n",
    "x_tgnr_resa = df_train_rst_clean.groupby(['Tågnr','resa'])['Tåguppdrag'].nunique()\n",
    "print(\"\\nNumber of Tågnr per Tåguppdrag:\")\n",
    "print(f\"df_train_rst_clean - max: {x_tgnr_resa.max()}, min: {x_tgnr_resa.min()}\")\n",
    "\n",
    "# Now checking how many Tågnr per Tåguppdrag\n",
    "x_tgupp_resa = df_train_rst_clean.groupby(['Tåguppdrag','resa'])['Tågnr'].nunique()\n",
    "print(\"\\nNumber of Tågnr per Tåguppdrag:\")\n",
    "print(f\"df_train_rst_clean - max: {x_tgupp_resa.max()}, min: {x_tgupp_resa.min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the unique identification is only possible using a combination of Tågnr and Tåguppdrag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting stops from train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before trying to find the closest line (line number/name) to a certain train (resa, i.e, tågnr-uppdrag-datum). Let us extract the stops.\n",
    "First, we append the stopping pattern information to our delayed trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with stops from 'Plats' column\n",
    "train_resa_stops = df_train_rst_clean.groupby(\n",
    "    [\n",
    "    'resa', 'Tågnr', 'Tåguppdrag', 'Tågsort','Start_uppdrag_sign', 'Slut_uppdrag_sign','Start_resa_sign', 'Slut_resa_sign'\n",
    "    ]\n",
    "    )['Plats_sign'].agg(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are duplicates in the 'Plats' lists, we can remove them while preserving order\n",
    "train_resa_stops['Plats_sign'] = train_resa_stops['Plats_sign'].apply(lambda x: list(dict.fromkeys(x)))\n",
    "train_resa_stops['Plats_len'] = train_resa_stops['Plats_sign'].apply(lambda x: len(x))\n",
    "\n",
    "# rename column Plats_sign to Stopps\n",
    "train_resa_stops.rename(columns={'Plats_sign': 'Stopps'}, inplace=True)\n",
    "\n",
    "# Convert lists to tuples to make them hashable\n",
    "train_resa_stops['Stopps'] = train_resa_stops['Stopps'].apply(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a reduced version of train_resa_stops with only Tåguppdrag and Plats while keeping the row with the longest list of stops, call it train_resa_stops_taguppdrag\n",
    "train_resa_stops = train_resa_stops.sort_values(by='Plats_len', ascending=False)\n",
    "\n",
    "# when there are duplicates in 'Tåguppdrag', keep the row with the highest 'Plats_len'\n",
    "train_stops_no_duplicates = train_resa_stops.drop_duplicates(subset=['Tågnr','Tåguppdrag','Tågsort','Start_uppdrag_sign','Slut_uppdrag_sign'], keep='first')\n",
    "\n",
    "\n",
    "\n",
    "#train_stops_no_duplicates = train_stops_no_duplicates[['resa','Tåguppdrag', 'Stopps']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Create a mapping DataFrame with unique combinations\n",
    "mapping_df = train_stops_no_duplicates[['Tågnr','Tåguppdrag', 'Tågsort', 'Start_uppdrag_sign', 'Slut_uppdrag_sign', 'Stopps']].drop_duplicates()\n",
    "\n",
    "# Merge the original DataFrame with the mapping DataFrame\n",
    "train_resa_stops_same = train_resa_stops.merge(\n",
    "    mapping_df[['Tågnr','Tåguppdrag', 'Tågsort', 'Start_uppdrag_sign', 'Slut_uppdrag_sign', 'Stopps']],\n",
    "    on=['Tågnr','Tåguppdrag', 'Tågsort', 'Start_uppdrag_sign', 'Slut_uppdrag_sign'],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Use the merged Stopps column to replace the original one\n",
    "train_resa_stops_same = train_resa_stops_same.drop(columns=['Stopps_x']).rename(columns={'Stopps_y': 'Stopps'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # there are rows in train_resa_stops with same Tåguppdrag but have different Plats_len\n",
    "# # how many rows are there?\n",
    "\n",
    "# # Count how many times each Tåguppdrag appears\n",
    "# duplicates = train_resa_stops.groupby('Tåguppdrag').size().reset_index(name='count')\n",
    "\n",
    "# # Filter to only show Tåguppdrag that appear more than once\n",
    "# multiple_entries = duplicates[duplicates['count'] > 1]\n",
    "\n",
    "# # Get the number of Tåguppdrag with multiple entries\n",
    "# num_duplicates = len(multiple_entries)\n",
    "\n",
    "# # To see the actual duplicate rows with their Plats_len:\n",
    "# duplicate_details = train_resa_stops[train_resa_stops['Tåguppdrag'].isin(multiple_entries['Tåguppdrag'])]\n",
    "# duplicate_details = duplicate_details[['Tåguppdrag', 'Plats_len']].sort_values('Tåguppdrag')\n",
    "\n",
    "# # Get the number of unique Plats_len for each Tåguppdrag\n",
    "# unique_plats_len = duplicate_details.groupby('Tåguppdrag')['Plats_len'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stops = train_stops_no_duplicates.drop_duplicates(subset=['Stopps'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AbdouAA\\AppData\\Local\\Temp\\ipykernel_22136\\3534091136.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  line_stops = df_line.groupby('Linje').apply(\n"
     ]
    }
   ],
   "source": [
    "# Group by 'Linje' and combine the 'från_sign' and 'till_sign' for each line\n",
    "line_stops = df_line.groupby('Linje').apply(\n",
    "    lambda x: list(x['från_sign']) + [x['till_sign'].iloc[-1]]\n",
    ").reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "line_stops.columns = ['Linje', 'Stopps']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching delayed trains to traffic lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now match delayed trains (subset with unique stop patterns) to the most likely traffic line. The most likely line is chosen as the one with the highest similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # type: ignore\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def calculate_score(train_stops, line_stops):\n",
    "    \"\"\"\n",
    "    Calculate a similarity score between train stops and line stops.\n",
    "    \"\"\"\n",
    "    # Match first and last stop\n",
    "    score = 0\n",
    "\n",
    "    if train_stops[0] == line_stops[0]:\n",
    "        score += 2  # Higher weight for matching first stop\n",
    "    if train_stops[-1] == line_stops[-1]:\n",
    "        score += 2  # Higher weight for matching last stop\n",
    "    \n",
    "    # Calculate sequence similarity for intermediate stops\n",
    "    sequence_similarity = SequenceMatcher(None, train_stops, line_stops).ratio()\n",
    "    score += sequence_similarity * 10  # Adjust weight for sequence similarity\n",
    "    \n",
    "    return score\n",
    "\n",
    "def get_inverted_line(line_id):\n",
    "    \"\"\"\n",
    "    Get the inverted line ID.\n",
    "    \"\"\"\n",
    "    return line_id[:-1] if line_id.endswith('R') else f\"{line_id}R\"\n",
    "\n",
    "def match_trains_to_lines(train_stops_df, line_stops_df):\n",
    "    \"\"\"\n",
    "    Match trains to lines based on similarity scores, including inverted stops.\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    for _, train_row in train_stops_df.iterrows():\n",
    "        best_score = -1\n",
    "        best_match = None\n",
    "        best_direction = 'Normal'\n",
    "\n",
    "        # if train_row has no stops, set best score to -1 and continue\n",
    "        if len(train_row['Stopps']) > 0:\n",
    "            for _, line_row in line_stops_df.iterrows():\n",
    "\n",
    "                # Calculate score for normal stops\n",
    "                normal_score = calculate_score(train_row['Stopps'], line_row['Stopps'])\n",
    "                \n",
    "                # Calculate score for inverted stops\n",
    "                inverted_stops = line_row['Stopps'][::-1]\n",
    "                inverted_score = calculate_score(train_row['Stopps'], inverted_stops)\n",
    "                \n",
    "                # Determine better match (normal or inverted)\n",
    "                if inverted_score > normal_score:\n",
    "                    current_score = inverted_score\n",
    "                    current_match = get_inverted_line(line_row['Linje'])\n",
    "                    current_direction = 'Inverted'\n",
    "                else:\n",
    "                    current_score = normal_score\n",
    "                    current_match = line_row['Linje']\n",
    "                    current_direction = 'Normal'\n",
    "                \n",
    "                # Update best match\n",
    "                if current_score > best_score:\n",
    "                    best_score = current_score\n",
    "                    best_match = current_match\n",
    "                    best_direction = current_direction\n",
    "        \n",
    "        matches.append({\n",
    "            'Stopps': train_row['Stopps'],\n",
    "            'Predicted_Line': best_match,\n",
    "            'Score': best_score,\n",
    "            'Direction': best_direction\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(matches)\n",
    "\n",
    "\n",
    "matching_result = match_trains_to_lines(train_stops, line_stops)\n",
    "\n",
    "# rename column Stopps to Stopps_train\n",
    "matching_result.rename(columns={'Stopps': 'Stopps_train'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to results corresponding stops of the predicted line\n",
    "matching_result_stops = pd.merge(matching_result, line_stops, left_on='Predicted_Line', right_on='Linje', how='left').rename(columns={'Stopps': 'Stopps_line'})\n",
    "matching_result_stops.drop(columns=['Linje'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can construct the final table where all the trains are identified with a specific traffic line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export matching_result_stops to Excel, keep only the columns 'Predicted_Line', 'Score', 'Stopps_line', 'Stopps_train'\n",
    "columns_to_keep = ['Predicted_Line', 'Score', 'Stopps_line', 'Stopps_train']\n",
    "matching_result_stops = matching_result_stops[columns_to_keep]\n",
    "matching_result_stops.to_excel('../data/output_data/matching_result_stops.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match back with Lupp data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the matching between train stopps and predicted lines. We need to use that to include a column called predicted line in our original train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge train_resa_stops (on Stopps) with matching_result_stops (on Stopps_train)\n",
    "train_resa_stops_predicted_lines = train_resa_stops_same.merge(matching_result_stops, left_on='Stopps', right_on='Stopps_train', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['resa', 'Tågnr', 'Tåguppdrag', 'Tågsort', 'Start_uppdrag_sign',\n",
       "       'Slut_uppdrag_sign', 'Start_resa_sign', 'Slut_resa_sign', 'Plats_len',\n",
       "       'Predicted_Line', 'Score', 'Stopps_line', 'Stopps_train'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove columns Stopps\n",
    "train_resa_stops_predicted_lines.drop(columns=['Stopps'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Merge train_resa_stops_same with train_resa_stops_predicted_lines to get predicted lines for all trains\n",
    "df_merged = train_resa_stops.merge(\n",
    "    train_resa_stops_predicted_lines[['resa','Tågnr','Tåguppdrag', 'Tågsort', 'Start_uppdrag_sign', 'Slut_uppdrag_sign', 'Predicted_Line', 'Score', 'Stopps_line']],\n",
    "    on=['resa','Tågnr','Tåguppdrag', 'Tågsort', 'Start_uppdrag_sign', 'Slut_uppdrag_sign'],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "df_merged.to_excel('../data/output_data/train_data_matched_lines.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
