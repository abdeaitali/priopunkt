{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching trains with traffic lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal here is to develop a function that makes it possible to identify the line of a specific delayed train. The reason why we need such a function is because the passenger ridership estimation is given per line where the delay data is per specific train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us import the dataset for all the traffic lines (used in the ridership estimation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import excel file static_pass_all_2024.xlsx\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "# read by default 1st sheet of an excel file\n",
    "df_line = pd.read_excel('static_pass_all_2024.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all the columns except the first 3 (no need for ridership data, only the line number, name and stopping patterns are of interest)\n",
    "df_line = df_line.iloc[:, :9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now import the train data, more specifically the trains that are affected by delays. Of interest here are particularly Tågnr\tand Tåguppdrag.\n",
    "The goal is to match all of them to a specific line number in df_line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read by default 1st sheet of an excel file\n",
    "df_train = pd.read_excel('metatraindata_2023.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also read the Lupp data where we have more attributes for each train, more particularly the stopping pattern. There are four different files in the data folder named as follows Rapport_T23_vX.csv where X is 11, 19, 28 and 37, we will read all of these and combine them in one dataframe, note the first row of each file is the header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Define the folder path and file pattern\n",
    "folder_path = 'data/'  # Adjust the folder path if needed\n",
    "file_pattern = 'Rapport_T23_v*.csv'\n",
    "\n",
    "# Use glob to find all matching files\n",
    "file_paths = glob.glob(folder_path + file_pattern)\n",
    "\n",
    "# Read all files into a list of DataFrames\n",
    "dfs = [pd.read_csv(file, header=0) for file in file_paths]\n",
    "\n",
    "# Combine all DataFrames into one\n",
    "df_lupp = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to clean up (make this df a bit smaller), e.g., by removing unnecessary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from combined_df remove the following columns\n",
    "# År (PAU)\n",
    "# Veckonr (PAU)\n",
    "# Datum (PAU)\n",
    "# Tågslag, but before remove all raws where Tågslag is not RST\n",
    "\n",
    "df_lupp_rst = df_lupp[df_lupp['Tågslag'] == 'RST']\n",
    "df_lupp_rst_clean = df_lupp_rst.drop(columns=['År (PAU)', 'Veckonr (PAU)', 'Datum (PAU)', 'Tågslag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all rows where both Uppehållstypavgång is Passage and Uppehållstypankomst is Passage\n",
    "df_lupp_rst_clean = df_lupp_rst_clean[(df_lupp_rst_clean['Uppehållstypavgång'] != 'Passage') | (df_lupp_rst_clean['Uppehållstypankomst'] != 'Passage')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many trains from df_train that are in df_lupp_rst_clean\n",
    "# for that search using the column Tågnr and Tåguppdrag from df_train\n",
    "# and use similar columns Tåguppdrag and Tågnr from df_lupp_rst_clean\n",
    "# to find the matching trains\n",
    "\n",
    "# make sure these are int in both dataframes\n",
    "df_train['Tågnr'] = df_train['Tågnr'].astype('Int64')\n",
    "df_train['Tåguppdrag'] = df_train['Tåguppdrag'].astype('Int64')\n",
    "df_lupp_rst_clean['Tåguppdrag'] = df_lupp_rst_clean['Tåguppdrag'].astype('Int64')\n",
    "\n",
    "# in df_lupp_rst_clean, remove spaces between numbers first in Tågnr\n",
    "# Remove spaces between numbers in the Tågnr column\n",
    "df_lupp_rst_clean['Tågnr'] = df_lupp_rst_clean['Tågnr'].astype(str).str.replace(r'\\s+', '', regex=True)\n",
    "df_lupp_rst_clean['Tågnr'] = df_lupp_rst_clean['Tågnr'].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "3 1\n"
     ]
    }
   ],
   "source": [
    "# for each Tågnr, print how many possible Tåguppdrag there are\n",
    "# this is to see if there are any duplicates in the data\n",
    "x = df_lupp_rst_clean.groupby('Tågnr')['Tåguppdrag'].nunique()\n",
    "y = df_train.groupby('Tågnr')['Tåguppdrag'].nunique()\n",
    "# print the max and min for each dataframe\n",
    "print(x.max(), x.min())\n",
    "print(y.max(), y.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching train delay and Lupp data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before trying to find the closest line (line number/name) to a certain train (tågnr/uppdrag). Let us first look att how many delayed trains can we identify in the sample of Lupp data that we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matching trains: 5833\n",
      "Out of 14474 unique trains\n",
      "Percentage of matching trains: 40.30%\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates from df_train and combined_df based on ('Tågnr', 'Tåguppdrag')\n",
    "df_train_test = df_train.drop_duplicates(subset=['Tågnr', 'Tåguppdrag'])\n",
    "combined_df_test = df_lupp_rst_clean.drop_duplicates(subset=['Tågnr', 'Tåguppdrag'])\n",
    "\n",
    "# Perform an inner merge to find matching trains\n",
    "matching_trains = pd.merge(\n",
    "    df_train_test, \n",
    "    combined_df_test, \n",
    "    how='inner', \n",
    "    left_on=['Tågnr', 'Tåguppdrag'], \n",
    "    right_on=['Tågnr', 'Tåguppdrag']\n",
    ")\n",
    "\n",
    "# Count the number of matching trains\n",
    "num_matching_trains = matching_trains.shape[0]\n",
    "print(f\"Number of matching trains: {num_matching_trains}\")\n",
    "\n",
    "# Count the number of unique trains in df_train\n",
    "num_unique_trains = len(df_train_test[['Tåguppdrag']])\n",
    "print(f\"Out of {num_unique_trains} unique trains\")\n",
    "\n",
    "# Calculate the percentage of matching trains\n",
    "matching_percentage = num_matching_trains / num_unique_trains * 100\n",
    "print(f\"Percentage of matching trains: {matching_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know that we have stopping pattern information (from Lupp data T23) for around 40% of the delayed trains (in metatraindata_2023). From now on, we focus on matching these 40% delayed trains to their line numbers.\n",
    "\n",
    "First, we append the stopping pattern information to our delayed trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_stops = df_lupp_rst_clean[\n",
    "    ((df_lupp_rst_clean['Uppehållstypavgång'].isin(['Uppehåll', 'Första']))) |\n",
    "    ((df_lupp_rst_clean['Uppehållstypankomst'].isin(['Sista'])))\n",
    "]\n",
    "\n",
    "first_dates = filtered_stops.groupby(['Tågnr', 'Tåguppdrag'])['Datum'].min().reset_index()\n",
    "filtered_stops = pd.merge(filtered_stops, first_dates, on=['Tågnr', 'Tåguppdrag', 'Datum'])\n",
    "\n",
    "stops_per_train = (\n",
    "    filtered_stops.groupby(['Tågnr', 'Tåguppdrag'], as_index=False)\n",
    "    .agg({'Delsträckanummer': list, 'Avgångplatssignatur': list, 'Uppehållstypankomst': list, 'AnkomstplatsPlatssignatur': list})\n",
    "    .apply(lambda x: pd.Series({\n",
    "        'Tågnr': x['Tågnr'],\n",
    "        'Tåguppdrag': x['Tåguppdrag'],\n",
    "        'Stopps': (\n",
    "            [stop for i, stop in zip(x['Delsträckanummer'], x['Avgångplatssignatur']) \n",
    "             if pd.notna(stop)] +\n",
    "            [x['AnkomstplatsPlatssignatur'][i] for i, type_a in enumerate(x['Uppehållstypankomst']) \n",
    "             if type_a == 'Sista' and pd.notna(x['AnkomstplatsPlatssignatur'][i])]\n",
    "        )\n",
    "    }), axis=1)\n",
    ")\n",
    "\n",
    "train_stops = pd.merge(\n",
    "    matching_trains, \n",
    "    stops_per_train, \n",
    "    how='inner', \n",
    "    on=['Tågnr', 'Tåguppdrag']\n",
    ")[['Tågnr', 'Tåguppdrag', 'Stopps']]\n",
    "\n",
    "# make sure all the stops are uppercase\n",
    "train_stops['Stopps'] = train_stops['Stopps'].apply(lambda x: [stop.upper() for stop in x])\n",
    "\n",
    "# some trains have the same stops, so remove duplicates\n",
    "train_stops_no_duplicates = train_stops.drop_duplicates(subset=['Stopps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AbdouAA\\AppData\\Local\\Temp\\ipykernel_17028\\3534091136.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  line_stops = df_line.groupby('Linje').apply(\n"
     ]
    }
   ],
   "source": [
    "# Group by 'Linje' and combine the 'från_sign' and 'till_sign' for each line\n",
    "line_stops = df_line.groupby('Linje').apply(\n",
    "    lambda x: list(x['från_sign']) + [x['till_sign'].iloc[-1]]\n",
    ").reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "line_stops.columns = ['Linje', 'Stopps']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching delayed trains to traffic lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now match delayed trains (subset with unique stop patterns) to the most likely traffic line. The most likely line is chosen as the one with the highest similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def calculate_score(train_stops, line_stops):\n",
    "    \"\"\"\n",
    "    Calculate a similarity score between train stops and line stops.\n",
    "    \"\"\"\n",
    "    # Match first and last stop\n",
    "    score = 0\n",
    "    if train_stops[0] == line_stops[0]:\n",
    "        score += 2  # Higher weight for matching first stop\n",
    "    if train_stops[-1] == line_stops[-1]:\n",
    "        score += 2  # Higher weight for matching last stop\n",
    "    \n",
    "    # Calculate sequence similarity for intermediate stops\n",
    "    sequence_similarity = SequenceMatcher(None, train_stops, line_stops).ratio()\n",
    "    score += sequence_similarity * 10  # Adjust weight for sequence similarity\n",
    "    \n",
    "    return score\n",
    "\n",
    "def get_inverted_line(line_id):\n",
    "    \"\"\"\n",
    "    Get the inverted line ID.\n",
    "    \"\"\"\n",
    "    return line_id[:-1] if line_id.endswith('R') else f\"{line_id}R\"\n",
    "\n",
    "def match_trains_to_lines(train_stops_df, line_stops_df):\n",
    "    \"\"\"\n",
    "    Match trains to lines based on similarity scores, including inverted stops.\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    for _, train_row in train_stops_df.iterrows():\n",
    "        best_score = -1\n",
    "        best_match = None\n",
    "        best_direction = 'Normal'\n",
    "        \n",
    "        for _, line_row in line_stops_df.iterrows():\n",
    "            # Calculate score for normal stops\n",
    "            normal_score = calculate_score(train_row['Stopps'], line_row['Stopps'])\n",
    "            \n",
    "            # Calculate score for inverted stops\n",
    "            inverted_stops = line_row['Stopps'][::-1]\n",
    "            inverted_score = calculate_score(train_row['Stopps'], inverted_stops)\n",
    "            \n",
    "            # Determine better match (normal or inverted)\n",
    "            if inverted_score > normal_score:\n",
    "                current_score = inverted_score\n",
    "                current_match = get_inverted_line(line_row['Linje'])\n",
    "                current_direction = 'Inverted'\n",
    "            else:\n",
    "                current_score = normal_score\n",
    "                current_match = line_row['Linje']\n",
    "                current_direction = 'Normal'\n",
    "            \n",
    "            # Update best match\n",
    "            if current_score > best_score:\n",
    "                best_score = current_score\n",
    "                best_match = current_match\n",
    "                best_direction = current_direction\n",
    "        \n",
    "        matches.append({\n",
    "            'Tågnr': train_row['Tågnr'],\n",
    "            'Predicted_Line': best_match,\n",
    "            'Score': best_score,\n",
    "            'Direction': best_direction\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(matches)\n",
    "\n",
    "# Example usage\n",
    "train_stops_df = train_stops_no_duplicates\n",
    "line_stops_df = line_stops\n",
    "\n",
    "result = match_trains_to_lines(train_stops_df, line_stops_df)\n",
    "\n",
    "# Add a column to results corresponding stops of the predicted line\n",
    "result = pd.merge(result, line_stops, left_on='Predicted_Line', right_on='Linje', how='left').rename(columns={'Stopps': 'Stopps_line'})\n",
    "result = pd.merge(result, train_stops_df, left_on='Tågnr', right_on='Tågnr', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can construct the final table where all the trains are identified with a specific traffic line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AbdouAA\\AppData\\Local\\Temp\\ipykernel_17028\\3374585376.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_stops_no_duplicates['Stopps'] = train_stops_no_duplicates['Stopps'].apply(tuple)\n"
     ]
    }
   ],
   "source": [
    "# Using the matching found in the previous step, i.e., in result, we want to add a column to the train_stops which include the predicted line\n",
    "# we have previously based the matching on the train stops with no duplicates train_stops_no_duplicates = train_stops.drop_duplicates(subset=['Stopps'])\n",
    "\n",
    "# Convert lists to tuples to make them hashable\n",
    "train_stops_no_duplicates['Stopps'] = train_stops_no_duplicates['Stopps'].apply(tuple)\n",
    "result['Stopps'] = result['Stopps'].apply(tuple)\n",
    "train_stops['Stopps'] = train_stops['Stopps'].apply(tuple)\n",
    "\n",
    "# Merge train_stops with result based on 'Stopps' to add predicted line\n",
    "train_stops_with_lines = pd.merge(\n",
    "    train_stops, \n",
    "    train_stops_no_duplicates[['Stopps']].merge(\n",
    "        result[['Stopps', 'Predicted_Line']],\n",
    "        on='Stopps',\n",
    "        how='left'\n",
    "    ),\n",
    "    on='Stopps',\n",
    "    how='left'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results to an excel file\n",
    "# * partial because the Lupp data that was used to identify the stops is only for some weeks in 2023, 40% of the delayed trains in 2023 have still been successfully identified!\n",
    "train_stops_with_lines.to_excel('train_stops_with_predicted_lines_partial_2023.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
