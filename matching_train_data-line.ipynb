{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching trains with traffic lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal here is to develop a function that makes it possible to identify the line of a specific delayed train. The reason why we need such a function is because the passenger ridership estimation is given per line where the delay data is per specific train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us import the dataset for all the traffic lines (used in the ridership estimation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import excel file static_pass_all_2024.xlsx\n",
    "import pandas as pd\n",
    "\n",
    "# read by default 1st sheet of an excel file\n",
    "df_line = pd.read_excel('static_pass_all_2024.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all the columns except the first 3 (no need for ridership data, only the line number, name and stopping patterns are of interest)\n",
    "df_line = df_line.iloc[:, :9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now import the train data, more specifically the trains that are affected by delays. Of interest here are particularly Tågnr\tand Tåguppdrag.\n",
    "The goal is to match all of them to a specific line number in df_line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ny datafil med alla RST tåg som har fått registrering av Infrastruktur händelser på södrastambanan\n",
    "df_train = pd.read_csv('train_data/traindata_2023_passenger_SSBevents_012025.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to clean up (make this df a bit smaller), e.g., by removing unnecessary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_rst = df_train[df_train['Tågslag'] == 'RST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the following columns\n",
    "# 'resa', 'Tågnr', 'Tåguppdrag', 'Plats', 'Tågslag', 'Tågsort', 'Aktivitetskod', 'Aktivitetskodbeskrivning'\n",
    "# 'PlanDatum', 'PlanTidpunkt', 'Datum',  'Tågläge', 'StartStation_resa', 'SlutStation_resa','StartStation_uppdrag', 'SlutStation_uppdrag\n",
    "df_train_rst_clean = df_train_rst[['resa', 'Tågnr', 'Tåguppdrag', 'Plats', 'Tågslag', 'Tågsort', 'Aktivitetskod', 'Aktivitetskodbeskrivning', 'PlanDatum', 'PlanTidpunkt', 'Datum',  'Tågläge', 'StartStation_resa', 'SlutStation_resa','StartStation_uppdrag', 'SlutStation_uppdrag']]\n",
    "df_train_rst_clean = df_train_rst_clean.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all rows where Aktivitetskodbeskrivning is not 'Påstigande av resande', or 'Av- och påstigande av resande' or 'Avstigande av resande'\n",
    "df_train_rst_clean = df_train_rst_clean[df_train_rst_clean['Aktivitetskodbeskrivning'].isin(['Påstigande av resande', 'Av- och påstigande av resande', 'Avstigande av resande'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, clean up df_plats_sign\n",
    "df_plats_sign = pd.read_excel('Förbindelselinje_2023_alla.xlsx')\n",
    "df_plats_sign = df_plats_sign.dropna(subset=['Plats'])\n",
    "df_plats_sign = df_plats_sign[['Plats', 'Plats_sign']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "# Define the regex matching function\n",
    "def match_station(station, plats_df):\n",
    "    station_variants = [\n",
    "        station,\n",
    "        re.sub(r'central', 'c', station, flags=re.IGNORECASE),\n",
    "        re.sub(r'(\\w+)( central)', r'\\1s central', station, flags=re.IGNORECASE),\n",
    "        re.sub(r'(\\w+)( c)', r'\\1s c', station, flags=re.IGNORECASE),\n",
    "        re.sub(r'(\\w+)s central', r'\\1 central', station, flags=re.IGNORECASE),\n",
    "        re.sub(r'(\\w+)s central', r'\\1 c', station, flags=re.IGNORECASE),\n",
    "    ]\n",
    "    \n",
    "    # if station is Marieholm, try matching Göteborg Marieholm\n",
    "    if station.lower() == 'marieholm':\n",
    "        station_variants.append('Göteborg Marieholm')\n",
    "    # if station is Helsingborg godsbangård, try matching Helsingborgs godsbangård\n",
    "    if station.lower() == 'helsingborg godsbangård':\n",
    "        station_variants.append('Helsingborgs godsbangård')\n",
    "    # if station is Hallsbergs pbg, try matching Hallsbergs personbangård\n",
    "    if station.lower() == 'hallsbergs pbg':\n",
    "        station_variants.append('Hallsbergs personbangård')\n",
    "\n",
    "    # if station is Stockholm Södra, try Stockholms Södra\n",
    "    if station.lower() == 'stockholm södra':\n",
    "        station_variants.append('Stockholms Södra')\n",
    "\n",
    "    # if station is Falkenbergs personstation, try matching Falkenberg personstation\n",
    "    if station.lower() == 'falkenbergs personstation':\n",
    "        station_variants.append('Falkenberg personstation')\n",
    "\n",
    "    # if station is Köpingebro , try matching f.d. Köpingebro\n",
    "    if station.lower() == 'köpingebro':\n",
    "        station_variants.append('f.d. Köpingebro')\n",
    "\n",
    "    for variant in station_variants:\n",
    "        matches = plats_df[plats_df['Plats'].str.match(re.escape(variant), case=False, na=False)]\n",
    "        if not matches.empty:\n",
    "            return matches['Plats_sign'].iloc[0]\n",
    "    return None\n",
    "\n",
    "# Filter activities first\n",
    "df_train_rst_clean = df_train_rst_clean[df_train_rst_clean['Aktivitetskodbeskrivning'].isin([\n",
    "    'Påstigande av resande', \n",
    "    'Av- och påstigande av resande', \n",
    "    'Avstigande av resande'\n",
    "])]\n",
    "\n",
    "# Create initial mapping for station signs\n",
    "df_train_rst_clean = df_train_rst_clean.merge(\n",
    "    df_plats_sign[['Plats', 'Plats_sign']], \n",
    "    left_on='Plats', \n",
    "    right_on='Plats', \n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle unmatched stations\n",
    "unmatched_stations = df_train_rst_clean[df_train_rst_clean['Plats_sign'].isna()]['Plats'].unique()\n",
    "\n",
    "# Apply regex matching for unmatched stations\n",
    "for station in unmatched_stations:\n",
    "    match = match_station(station, df_plats_sign)\n",
    "    if match:\n",
    "        df_train_rst_clean.loc[df_train_rst_clean['Plats'] == station, 'Plats_sign'] = match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the additional station mapping file\n",
    "df_plats_sign_pos = pd.read_excel('Plats_sign_pos.xlsx')\n",
    "\n",
    "# rename column Signatur to Plats_sign\n",
    "df_plats_sign_pos.rename(columns={'Signatur': 'Plats_sign'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third attempt: Regex matching using additional file for remaining unmatched stations\n",
    "unmatched_stations = df_train_rst_clean[df_train_rst_clean['Plats_sign'].isna()]['Plats'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station in unmatched_stations:\n",
    "    match = match_station(station, df_plats_sign_pos)\n",
    "    if match:\n",
    "        df_train_rst_clean.loc[df_train_rst_clean['Plats'] == station, 'Plats_sign'] = match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all station signs to uppercase\n",
    "df_train_rst_clean['Plats_sign'] = df_train_rst_clean['Plats_sign'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the full name for unmatched stations\n",
    "df_train_rst_clean.loc[df_train_rst_clean['Plats_sign'].isna(), 'Plats_sign'] = df_train_rst_clean.loc[df_train_rst_clean['Plats_sign'].isna(), 'Plats']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting stops from train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before trying to find the closest line (line number/name) to a certain train (resa, i.e, tågnr-uppdrag-datum). Let us extract the stops.\n",
    "First, we append the stopping pattern information to our delayed trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with stops from 'Plats' column\n",
    "train_resa_stops = df_train_rst_clean.groupby(['resa', 'Tåguppdrag'])['Plats_sign'].agg(list).reset_index()\n",
    "\n",
    "# If there are duplicates in the 'Plats' lists, we can remove them while preserving order\n",
    "train_resa_stops['Plats_sign'] = train_resa_stops['Plats_sign'].apply(lambda x: list(dict.fromkeys(x)))\n",
    "\n",
    "# create a reduced version of train_resa_stops with only Tåguppdrag and Plats while keeping the row with the longest list of stops, call it train_resa_stops_taguppdrag\n",
    "train_resa_stops['Plats_len'] = train_resa_stops['Plats_sign'].apply(lambda x: len(x))\n",
    "train_resa_stops = train_resa_stops.sort_values(by='Plats_len', ascending=False)\n",
    "\n",
    "# rename column Plats_sign to Stopps\n",
    "train_resa_stops.rename(columns={'Plats_sign': 'Stopps'}, inplace=True)\n",
    "\n",
    "\n",
    "train_stops_no_duplicates = train_resa_stops.drop_duplicates(subset='Tåguppdrag', keep='first')\n",
    "train_stops_no_duplicates = train_stops_no_duplicates[['Tåguppdrag', 'Stopps']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AbdouAA\\AppData\\Local\\Temp\\ipykernel_28528\\3534091136.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  line_stops = df_line.groupby('Linje').apply(\n"
     ]
    }
   ],
   "source": [
    "# Group by 'Linje' and combine the 'från_sign' and 'till_sign' for each line\n",
    "line_stops = df_line.groupby('Linje').apply(\n",
    "    lambda x: list(x['från_sign']) + [x['till_sign'].iloc[-1]]\n",
    ").reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "line_stops.columns = ['Linje', 'Stopps']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching delayed trains to traffic lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now match delayed trains (subset with unique stop patterns) to the most likely traffic line. The most likely line is chosen as the one with the highest similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def calculate_score(train_stops, line_stops):\n",
    "    \"\"\"\n",
    "    Calculate a similarity score between train stops and line stops.\n",
    "    \"\"\"\n",
    "    # Match first and last stop\n",
    "    score = 0\n",
    "\n",
    "    if train_stops[0] == line_stops[0]:\n",
    "        score += 2  # Higher weight for matching first stop\n",
    "    if train_stops[-1] == line_stops[-1]:\n",
    "        score += 2  # Higher weight for matching last stop\n",
    "    \n",
    "    # Calculate sequence similarity for intermediate stops\n",
    "    sequence_similarity = SequenceMatcher(None, train_stops, line_stops).ratio()\n",
    "    score += sequence_similarity * 10  # Adjust weight for sequence similarity\n",
    "    \n",
    "    return score\n",
    "\n",
    "def get_inverted_line(line_id):\n",
    "    \"\"\"\n",
    "    Get the inverted line ID.\n",
    "    \"\"\"\n",
    "    return line_id[:-1] if line_id.endswith('R') else f\"{line_id}R\"\n",
    "\n",
    "def match_trains_to_lines(train_stops_df, line_stops_df):\n",
    "    \"\"\"\n",
    "    Match trains to lines based on similarity scores, including inverted stops.\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    for _, train_row in train_stops_df.iterrows():\n",
    "        best_score = -1\n",
    "        best_match = None\n",
    "        best_direction = 'Normal'\n",
    "\n",
    "        # if train_row has no stops, set best score to -1 and continue\n",
    "        if len(train_row['Stopps']) > 0:\n",
    "            for _, line_row in line_stops_df.iterrows():\n",
    "\n",
    "                # Calculate score for normal stops\n",
    "                normal_score = calculate_score(train_row['Stopps'], line_row['Stopps'])\n",
    "                \n",
    "                # Calculate score for inverted stops\n",
    "                inverted_stops = line_row['Stopps'][::-1]\n",
    "                inverted_score = calculate_score(train_row['Stopps'], inverted_stops)\n",
    "                \n",
    "                # Determine better match (normal or inverted)\n",
    "                if inverted_score > normal_score:\n",
    "                    current_score = inverted_score\n",
    "                    current_match = get_inverted_line(line_row['Linje'])\n",
    "                    current_direction = 'Inverted'\n",
    "                else:\n",
    "                    current_score = normal_score\n",
    "                    current_match = line_row['Linje']\n",
    "                    current_direction = 'Normal'\n",
    "                \n",
    "                # Update best match\n",
    "                if current_score > best_score:\n",
    "                    best_score = current_score\n",
    "                    best_match = current_match\n",
    "                    best_direction = current_direction\n",
    "        \n",
    "        matches.append({\n",
    "            'Tåguppdrag': train_row['Tåguppdrag'],\n",
    "            'Predicted_Line': best_match,\n",
    "            'Score': best_score,\n",
    "            'Direction': best_direction\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(matches)\n",
    "\n",
    "# Example usage\n",
    "train_stops_df = train_stops_no_duplicates\n",
    "line_stops_df = line_stops\n",
    "\n",
    "result = match_trains_to_lines(train_stops_df, line_stops_df)\n",
    "\n",
    "# Add a column to results corresponding stops of the predicted line\n",
    "result = pd.merge(result, line_stops, left_on='Predicted_Line', right_on='Linje', how='left').rename(columns={'Stopps': 'Stopps_line'})\n",
    "result = pd.merge(result, train_stops_df, left_on='Tåguppdrag', right_on='Tåguppdrag', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can construct the final table where all the trains are identified with a specific traffic line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Stopps'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Stopps'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[185], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStopps\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStopps\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mtuple\u001b[39m)\n\u001b[0;32m      8\u001b[0m train_stops \u001b[38;5;241m=\u001b[39m train_resa_stops\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m----> 9\u001b[0m train_stops[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStopps\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_stops\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mStopps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mtuple\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Merge train_stops with result based on 'Stopps' to add predicted line\u001b[39;00m\n\u001b[0;32m     12\u001b[0m train_stops_with_lines \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(\n\u001b[0;32m     13\u001b[0m     train_stops, \n\u001b[0;32m     14\u001b[0m     train_stops_no_duplicates[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStopps\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mmerge(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     21\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Stopps'"
     ]
    }
   ],
   "source": [
    "# Convert lists to tuples to make them hashable\n",
    "train_stops_no_duplicates = train_stops_no_duplicates.copy()\n",
    "train_stops_no_duplicates['Stopps'] = train_stops_no_duplicates['Stopps'].apply(tuple)\n",
    "\n",
    "result = result.copy()\n",
    "result['Stopps'] = result['Stopps'].apply(tuple)\n",
    "\n",
    "train_stops = train_resa_stops.copy()\n",
    "train_stops['Stopps'] = train_stops['Stopps'].apply(tuple)\n",
    "\n",
    "# Merge train_stops with result based on 'Stopps' to add predicted line\n",
    "train_stops_with_lines = pd.merge(\n",
    "    train_stops, \n",
    "    train_stops_no_duplicates[['Stopps']].merge(\n",
    "        result[['Stopps', 'Predicted_Line']],\n",
    "        on='Stopps',\n",
    "        how='left'\n",
    "    ),\n",
    "    on='Stopps',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results to an excel file\n",
    "train_stops_with_lines.to_excel('train_stops_with_predicted_lines_all_2023.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
